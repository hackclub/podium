# PostgreSQL Migration: Technical Details

See [overview.md](./overview.md) for status and phases.

---

## Local Development

```bash
# Start local Postgres (one-time)
podman run -d --name podium-pg \
  -e POSTGRES_PASSWORD=localpass \
  -e POSTGRES_DB=podium \
  -p 5432:5432 \
  postgres:17

# Run migrations
doppler run --config dev -- uv run alembic upgrade head

# Start backend
doppler run --config dev -- uv run podium
```

All secrets come from Doppler. No `.secrets.toml` or `.env` files.

---

## SQLModel Models

**Location:** `backend/podium/db/postgres/`

| File | Purpose |
|------|---------|
| `__init__.py` | Exports all models, utilities, and typed query helpers |
| `base.py` | Async engine, session factory, typed query helpers |
| `links.py` | Junction tables for M2M relationships |
| `user.py` | User model + API schemas |
| `event.py` | Event model + API schemas |
| `project.py` | Project model + API schemas |
| `vote.py` | Vote model |
| `referral.py` | Referral model |

### Key Patterns

**IDs are `None` until saved:**
```python
id: UUID | None = Field(default=None, primary_key=True)
```

**M2M requires explicit link tables with surrogate PKs:**
```python
# In links.py - uses surrogate PK for Mathesar Extend compatibility
class EventAttendeeLink(SQLModel, table=True):
    __table_args__ = (UniqueConstraint("event_id", "user_id"),)
    
    id: UUID = Field(default_factory=uuid4, primary_key=True)  # Surrogate PK
    event_id: UUID = Field(foreign_key="events.id")
    user_id: UUID = Field(foreign_key="users.id")

# In event.py
attendees: list["User"] = Relationship(
    back_populates="events_attending", link_model=EventAttendeeLink
)
```

> **Why surrogate PKs?** Mathesar's "Extend" feature (Airtable-like linked record views)
> only works with junction tables that have exactly 3 columns: 1 single-column PK + 2 FKs.
> Composite PKs are not supported. See migration `e10bb6a39187`.

**`airtable_id` for migration mapping (temporary):**
```python
airtable_id: str | None = Field(default=None, max_length=32, unique=True, index=True)
```

> This field is only needed during migration for cross-referencing. Remove after
> migration is verified—just delete from models and run `alembic revision --autogenerate`.

---

## Alembic Commands

```bash
# Generate migration from model changes
doppler run --config dev -- uv run alembic revision --autogenerate -m "description"

# Apply all migrations
doppler run --config dev -- uv run alembic upgrade head

# Rollback one migration
doppler run --config dev -- uv run alembic downgrade -1

# Check if models match database
doppler run --config dev -- uv run alembic check
```

### Best Practices

- **Never edit applied migrations** — They represent history. Create new migrations for changes.
- **Review autogenerated code** — Alembic guesses intent; verify `upgrade()` and `downgrade()` are correct.
- **Use descriptive names** — `add_user_phone_field` not `update_models`.
- **Test downgrade** — Run `alembic downgrade -1` then `upgrade head` locally before pushing.

### Merge Conflicts

If two branches create migrations with the same `down_revision`:

```
Before merge (conflict):
  abc123 (base)
    ├── def456 (branch A) — down_revision = 'abc123'
    └── ghi789 (branch B) — down_revision = 'abc123'  ← CONFLICT

After fix (chain them):
  abc123 (base)
    └── def456 (branch A) — down_revision = 'abc123'
        └── ghi789 (branch B) — down_revision = 'def456'  ← FIXED
```

1. Keep both migration files
2. Open the second migration file (e.g., `ghi789_...py`)
3. Change `down_revision = 'abc123'` to `down_revision = 'def456'`
4. Run `alembic upgrade head` to verify the chain works

---

## Query Patterns (SQLModel)

### Typed Query Helpers

Use the typed helpers in `db/postgres/base.py` for better typing:

```python
from podium.db.postgres import scalar_one_or_none, scalar_all

# Get single result (properly typed)
event = await scalar_one_or_none(session, select(Event).where(Event.slug == slug))

# Get all results (properly typed)
projects = await scalar_all(session, select(Project).where(Project.event_id == event_id))
```

### Primary Key Lookups

Use `session.get()` for PK lookups (better typing and performance):

```python
# ✅ Preferred - uses session.get()
event = await session.get(Event, event_id)

# ❌ Avoid for PK lookups
result = await session.execute(select(Event).where(Event.id == event_id))
event = result.scalars().first()
```

### Creating Models

Use `Model.model_validate()` with `update` parameter:

```python
# ✅ Preferred - uses model_validate with update
new_event = Event.model_validate(
    event_create,  # EventCreate instance
    update={
        "slug": computed_slug,
        "join_code": join_code,
        "owner_id": user.id,
    },
)
session.add(new_event)
await session.commit()
await session.refresh(new_event)

# ❌ Avoid - manual field assignment
new_event = Event(
    name=event_create.name,
    description=event_create.description,
    slug=computed_slug,
    join_code=join_code,
    owner_id=user.id,
    # ... many more fields
)
```

### Updating Models

Use `sqlmodel_update()` with `model_dump(exclude_unset=True, exclude_none=True)`:

```python
# ✅ Preferred - uses sqlmodel_update
update_data = event_update.model_dump(exclude_unset=True, exclude_none=True)
event.sqlmodel_update(update_data)
await session.commit()

# ❌ Avoid - manual loop
for key, value in event_update.model_dump(exclude_unset=True).items():
    if value is not None:
        setattr(event, key, value)
```

### Eager Loading (avoid N+1)

```python
from sqlalchemy.orm import selectinload

# Load event with its attendees
stmt = (
    select(Event)
    .where(Event.id == event_id)
    .options(selectinload(Event.attendees))
)
event = await scalar_one_or_none(session, stmt)
# event.attendees is now loaded, no extra query
```

### M2M Operations

```python
# Add attendee to event (no session.add() needed for tracked objects)
event.attendees.append(user)
await session.commit()

# Remove attendee
event.attendees = [a for a in event.attendees if a.id != user_id]
await session.commit()

# Check membership
is_attending = user in event.attendees
```

---

## Doppler Configuration

| Variable | `dev` | `prod` |
|----------|-------|--------|
| `PODIUM_DATABASE_URL` | `postgresql+asyncpg://postgres:localpass@localhost:5432/podium` | `postgresql+asyncpg://...@coolify/podium` |
| `PODIUM_ENV` | `development` | `production` |

Dynaconf strips the `PODIUM_` prefix, so use `settings.database_url` in code.

**Required settings:**
- `database_url` - PostgreSQL connection string
- `jwt_secret` - JWT signing secret
- `loops_transactional_id` - Loops email template ID

**Optional settings (for migration only):**
- `airtable_token`, `airtable_base_id`, etc. - Only needed for `migrate_from_airtable.py`

---

## Backfill Script ✅

**Location:** `backend/scripts/migrate_from_airtable.py`

The script:
1. Connects to both Airtable (via PyAirtable) and Postgres (via SQLModel)
2. Iterates through Airtable records in dependency order
3. Creates Postgres rows, mapping `airtable_id` for idempotency
4. Handles M2M relationships after base records exist

**Migration order** (foreign keys must resolve):
1. Users
2. Events  
3. Event attendees (M2M)
4. Projects
5. Project collaborators (M2M)
6. Referrals
7. Votes

---

## Router Patterns ✅

All routers use these patterns:

```python
from fastapi import Depends
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession  # Use SQLModel's AsyncSession
from podium.db.postgres import User, Event, get_session, scalar_one_or_none

@router.get("/{event_id}")
async def get_event(
    event_id: Annotated[UUID, Path(title="Event ID")],
    session: Annotated[AsyncSession, Depends(get_session)],
) -> EventPublic:
    # Use session.get() for PK lookups
    event = await session.get(Event, event_id)
    if not event:
        raise HTTPException(status_code=404, detail="Event not found")
    return EventPublic.model_validate(event)

@router.post("/")
async def create_event(
    event: EventCreate,
    user: Annotated[User, Depends(get_current_user)],
    session: Annotated[AsyncSession, Depends(get_session)],
):
    # Use model_validate for creation
    new_event = Event.model_validate(event, update={"owner_id": user.id, ...})
    session.add(new_event)
    await session.commit()
    await session.refresh(new_event)
    return {"id": str(new_event.id)}

@router.put("/{event_id}")
async def update_event(
    event_id: UUID,
    event_update: EventUpdate,
    session: Annotated[AsyncSession, Depends(get_session)],
):
    event = await session.get(Event, event_id)
    # Use sqlmodel_update for updates
    update_data = event_update.model_dump(exclude_unset=True, exclude_none=True)
    event.sqlmodel_update(update_data)
    await session.commit()
    return {"message": "Event updated"}
```

---

## Cutover Checklist

### Before Cutover
- [x] Backfill script tested locally with real Airtable data
- [x] All routers refactored to use SQLModel
- [x] Code quality cleanup (typed helpers, model_validate, sqlmodel_update)
- [ ] E2E tests pass against Postgres
- [ ] Frontend OpenAPI client regenerated

### Cutover Day
1. Export Airtable data (CSV backup)
2. Run migrations on prod
3. Run backfill script
4. Deploy v2
5. Regenerate frontend client: `cd frontend && bun run openapi-ts`
6. Smoke test core flows
7. Monitor for 24-48 hours

### Post-Cutover
- [x] Delete cache layer code
- [ ] Remove Redis/Valkey from Coolify
- [ ] Remove PyAirtable dependency
- [ ] Delete old Airtable models
- [ ] Remove `airtable_id` from models + generate migration (also fixes Mathesar record summary auto-detection)
